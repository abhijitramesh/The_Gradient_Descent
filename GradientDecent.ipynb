{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sigma{(x)} & = 1/1+e^{-x}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/1+np.exp(-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative of sigmoid function we get\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial }{\\partial x}  \\sigma{(x)}  = {\\sigma{(x)}} * (1-\\sigma{(x)})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * 1-(sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnrate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of neural network would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_output = sigmoid(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \\begin{align}\n",
    " Error =  y - {y\\widehat{}}\n",
    "         \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "\\begin{equation*}\n",
    "y\\widehat{}\n",
    "\\end{equation*}\n",
    "is actually nn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - nn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of output can be found by passing the output(*h*) through the derivative of sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_grad = sigmoid_prime(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \\begin{align}\n",
    " \\delta =  Error * output grad\n",
    "         \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_term = error*output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let us perfrom Gradient Decent Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to implement the same for 4 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.dot(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_output=sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - nn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_grad = sigmoid_prime(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_term = error * output_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_w = learnrate * error_term * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "1.4493289641172216\n",
      "Amount of Error:\n",
      "-0.9493289641172216\n",
      "Change in Weights:\n",
      "[-0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent in Actual Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen how to update the weights for a single iteration but we need to do the same for multiple epochs\n",
    "Let us use this [http://www.ats.ucla.edu/stat/data/binary.cs](https://stats.idre.ucla.edu/stat/data/binary.csv) dataset for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset given here have three input features GRE score GPA and rank of the undergrad school.\n",
    "The goal here is to predict if the student gets accepted to the university with the given criterion we will use sigmoid funtion here also for output activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the dataset now you will notice that the column rank is not hot encoded correctly,\n",
    "Rank 2 is not twice Rank 1 or Rank 3 is not thrice Rank 1 or anything like that.\n",
    "\n",
    "For this we need to use dummyvariables and split the ranks into four columns and then use 0 or 1 to represent them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets read the data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = pd.read_csv('binary.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the encoding in rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to standadize the gre and gpa since they are fairly large values. \n",
    "We are using sigmoid activation function and this would squash fairly large and small values.\n",
    "The gradient of this would become 0 and the gradient descent step would go to zero and die off.\n",
    "We should standardize the values by keeping a mean of 0 and standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhijitramesh/miniconda3/envs/gradientdescent/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using large data here using, SSE would be hard since the gradient would decrease very fast and the training won't be proper. We have to compensate this by using a very small learning rate. We can also use Mean Square Error menthod and the learning rate would be in the range of 0.01 and 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \\begin{align}\n",
    " Error = \\frac{1}{2m} & \\sum_{\\mu} ({y^{\\mu} - y\\widehat{}^\\mu})^2\n",
    "         \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to repeat the following steps for e epohs:\n",
    "\n",
    "1. Set the change in weight to zero \\begin{equation*} \\nabla w_i = 0 \\end{equation*}\n",
    "2. For record in training data:\n",
    "\n",
    "    * We calculate the output for \\begin{equation*} y\\widehat{} = f(\\sum_i w_ix_i) \\end{equation*}\n",
    "    * Calculate the error term for output \\begin{equation*} \\delta = (y - y\\widehat{}) * f^| (\\sum_i w_i x_i)\\end{equation*}\n",
    "    * Update the weigth step \\begin{equation*} w_i = w_i + \\eta \\delta x_i\\end{equation*}\n",
    "    \n",
    "3. Update the weights \\begin{equation*} w_i = w_i + \\eta \\nabla w_i / m\\end{equation*} Here n is the learning rate and m in the number of records.(Averaging the weights to avoid large variations of training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seed to make debugging easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records, n_features = features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_loss = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use random values for the weigths since we need them to have different starting values, breaking system. We need it to be small becuase the input to sigmoid is in the linear region near 0 and not squashed at higer and lower ends.\n",
    "We will initalisze the weights from a normal distribution centered at 0. A good scale is 1/\n",
    "root(n) where n is the number of input units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learnrate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n",
      "Train loss:  nan\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(x,weights))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error*output*(1-output)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate*del_w/n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean(((out - targets) **2).astype(np.float128))\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhijitramesh/miniconda3/envs/gradientdescent/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
