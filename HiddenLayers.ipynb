{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a normal classfiation problem such as classifying a red and blue points reguarding some criterial if the problem is linear we just have to draw one line. This can be achinved by a normal perspectron but imagine if the solution is a curve if we need to achieve this we need a more complex neural network. One way to go about this would be to combine two lines which means combining two pespectrons.\n",
    "\n",
    "### Layer 1\n",
    "Would be the input\n",
    "\n",
    "### Layer 2 \n",
    "Would recive the inputs calulate some function and apply an activation funciton on it and would go to next layer.\n",
    "\n",
    "### Layer 3\n",
    "Would recive the updated values with activation applied which would act as the weights to this layer and again would calulate some function to which also the activation function is applied and then this is given as output.\n",
    "\n",
    "Here we are stacking up layer on top of layer which would allow the network to *learn* more complex patterns and hence we call this type of learning *deep learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a netork with three input layers, two hidden layers and one output layer.\n",
    "\n",
    "The input would have weights which would be names as w<sub>ij</sub>\n",
    "\n",
    "   Where ij is representing the dimestion of a matrix how?\n",
    "Well let us consider the first input node it would have two weights w<sub>11</sub> and w<sub>12</sub>, which represents weights going from the input layer 1 to the hidden layer 1 ie.. w<sub>11</sub> and the weights going from input layer 1 to hidden layer 2 ie.. w<sub>12</sub>.\n",
    "\n",
    "Similarly \n",
    "\n",
    "w<sub>21</sub> represents the weights going from input layer 2 to the hidden layer 1,\n",
    "w<sub>22</sub> represents the weights going from input layer 2 to the hidden layer 2,\n",
    "w<sub>31</sub> represents the weights going from input layer 3 to the hidden layer 1 and\n",
    "w<sub>32</sub> represents the weights going from input layer 3 to the hidden layer 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we were not able to represent this as a matrix but an array or vector now we can represent this as a 2d matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have the theory right we should be able to implement this in a 4x3x2 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23413696  1.57921282  0.76743473 -0.46947439]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(4)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have out features X with 4 random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02257763  0.00675282 -0.14247482]\n",
      " [-0.05443827  0.01109226 -0.11509936]\n",
      " [ 0.0375698  -0.06006387 -0.02916937]\n",
      " [-0.06017066  0.18522782 -0.00134972]]\n"
     ]
    }
   ],
   "source": [
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "print(weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10577109  0.08225449]\n",
      " [-0.12208436  0.02088636]\n",
      " [-0.19596701 -0.1328186 ]]\n"
     ]
    }
   ],
   "source": [
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "print(weights_hidden_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the weights required we are ready to make a foward pass through the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_in = np.dot(X,weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done the foward pass and did a dot product between the features(X) and weights to the input layer now we need to apply sigmoid to this to find the output of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49409967 0.47075371 0.45756242]\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "print(hidden_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the output a vector with 3 values Great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out the input to the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_in = np.dot(hidden_layer_out,weights_hidden_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we did one more foward pass Let us calulate the output of the output layer by applying the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_out = sigmoid(output_layer_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45031445 0.49742538]\n"
     ]
    }
   ],
   "source": [
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Great_ We have successfully implemented HiddenLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitae835acd69ed4a8fb4382af95f15e85a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
